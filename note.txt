Thanks — I read all three files carefully and thought through real-world usage, correctness, leakage, scalability, evaluation and production concerns. Short answer first:

# Verdict (short)

The project is *close* to being a usable real-world recommender, but **not yet production-ready**. It will run and train, but it currently contains **data-leakage issues, grouping/ordering bugs, scalability bottlenecks (SBERT embedding compute), and some feature / evaluation gaps** that will reduce accuracy and make offline metrics optimistic. With the fixes and improvements I list below it can become a robust real-world buyer–vehicle recommender.

---

# High-priority problems (must-fix)

## 1) **Data leakage through user profiles / feature engineering**

Where it matters most: `model_utils.build_user_profiles()` builds user profile embeddings from `df` rows with `matched==1`. In the current flow you call `build_features_from_candidates(df)` **before any train/test split**. That means user profile embeddings are built using **the same rows** that will become training examples — including examples that should be held out for testing. This produces leakage: features (profile_similarity) implicitly see label information or future matches and will inflate train/test metrics.

**Fix**: Split users (or time slices) into train vs test *first*, and compute user profiles only from the historical matches available to the model at training time. For evaluation, profile features for test users must be computed only from earlier history (not from the held-out candidate you are evaluating). See code patch below.

---

## 2) **Group sizes for LGBMRanker may be incorrect / ordering issues**

In `train_real.py` you attempt to sort and produce group sizes. There are two mistakes:

* `user_ids.value_counts(sort=False).sort_index().to_list()` will produce counts sorted by user_id *value* rather than in the order the groups appear in X_train. LightGBM expects `group` array to match the row ordering (i.e. sizes in the order rows appear).
* You compute features on the full df and then later reorder via `X = X.loc[df_sorted.index]` — this can work but is fragile.

**Fix**: After you split into train/test and produce X_train (rows in the final order passed to model.fit), compute `group_train` by grouping **preserving order**:

```python
# user_train is a pandas Series aligned with X_train
group_train = user_train.groupby(user_train, sort=False).size().tolist()
group_test  = user_test.groupby(user_test, sort=False).size().tolist()
```

Or explicitly compute sizes by walking the ordered user_id Series.

---

## 3) **Feature engineering run-time & memory: SBERT is computed too often and not cached**

`compute_text_embeddings` is called for buyer_text and vehicle_text for *every candidate pair* in `build_features_from_candidates`. For a large candidate table (hundreds of thousands / millions), this will be **very slow** and memory-heavy. Likewise `build_user_profiles` recomputes embeddings again.

**Fixes (practical)**:

* Precompute and persist **unique** vehicle embeddings and unique buyer (pre_order) embeddings once, then join/embed by id for candidate generation.
* Use batching in `SentenceTransformer.encode(..., batch_size=..., show_progress_bar=True, device=...)`.
* Save embeddings to disk (parquet / npy) and reuse for training and inference.
* Consider approximate nearest neighbor (FAISS / Annoy) for profile matching rather than computing dot product to all vehicles on the fly.

Example: compute `veh_embeddings_map = {veh_id: vector}` once and map to candidate rows using merge/join, then compute similarity via dot product.

---

## 4) **User profiles: naive averaging and leakage**

`build_user_profiles` averages vehicle embeddings for each user. Averaging is OK as a baseline, but:

* It ignores recency / weight of records (older purchases may be less relevant).
* It includes *current* match unless you exclude it (leakage, see point 1).
* For users with few matches the average vector will be noisy; you use an arbitrary fallback `0.5` when no profile exists.

**Improvements**:

* Use time-weighted averaging or attention-weighted combining.
* Use a principled fallback (e.g., mean population embedding or brand-category prior) instead of hard-coded 0.5.
* For numeric stability, ensure all profile vectors and vehicle vectors are normalized.

---

## 5) **Label creation / class imbalance**

`etl.py` sets `matched = (matched_vehicle_id == vehicle_id)`. That is fine if `matched_vehicle_id` comes from a correct ground-truth mapping. But check:

* Are there duplicate candidate rows? If many negatives per positive, class imbalance will be large.
* LGBMRanker expects relevance labels; treat matched as relevance score (1) and others 0. Consider weighting or sampling.

**Recommendation**: retain all negatives for ranking, but evaluate with ranking metrics (NDCG, MAP@K) and possibly upweight positives or sample negatives carefully.

---

## 6) **Modeling choices: LGBMRanker is reasonable but requires correct grouping and evaluation**

`LGBMRanker(objective='lambdarank')` is appropriate. Make sure:

* Group sizes reflect the number of candidates per query (user) in the *order you feed the model*.
* Use `ndcg@k` with k matching production top-K (e.g., ndcg@5 if you show 5 buyers).
* Use group-aware cross-validation (group K-fold by user) for robust estimation.

---

## 7) **Other coding bugs / small issues**

* `etl.get_sql()` prints and re-raises on FileNotFoundError — OK, but consider raising a descriptive error.
* `label_candidates` casts to `str` then compares; handle NaNs explicitly to avoid false matches.
* In `model_utils.cosine_similarity`, ensure both inputs are normalized (you do) and handle shape errors if vectors are 1-D.

---

# Concrete improvements & code snippets

### A — Split users before feature engineering (critical)

Replace current workflow: compute features on full df -> split -> train with leakage
With: split unique users -> compute features for train and test separately (profile features only from train history) -> train.

Example modifications in `train_real.py` (pseudo):

```python
# Load df
df = pd.read_parquet(parquet_path)

# split users
unique_users = df['user_id'].unique()
train_users, test_users = train_test_split(unique_users, test_size=test_size, random_state=RANDOM_STATE)

# train_df and test_df are subsets of the candidate pairs
train_df = df[df['user_id'].isin(train_users)].reset_index(drop=True)
test_df  = df[df['user_id'].isin(test_users)].reset_index(drop=True)

# Build features for train:
X_train = build_features_from_candidates(train_df, historical_matches=train_df[train_df['matched']==1])
# Build features for test: historical matches should come ONLY from train history
X_test  = build_features_from_candidates(test_df, historical_matches=train_df[train_df['matched']==1])
```

To support this change update `build_features_from_candidates` signature to accept `historical_matches` (a DataFrame of past matches used to compute profiles), and **do not** build profiles from the candidates you are currently featurizing.

### B — Change build_user_profiles to accept historical data and support leave-one-out

Modify `build_user_profiles(historical_matches_df)` where historical_matches_df contains only earlier matched records. For leave-one-out when you have a match in the train set and you're building features for that same user/vehicle pair, you must exclude the current pair if it is in historical data. This is most relevant for building features for training instances when historical data includes that matched pair — but simpler approach: historical_matches only contains matches prior to evaluation time (time-based splitting) or just use train distinct matches for train, and train matches for test as well.

### C — Efficient embedding flow

* Precompute embeddings for all unique `veh_brand + model` (vehicles) and for unique `pre_brand + pre_model` (preorders).
* Save maps: `vehicle_id -> embedding` and `pre_order_id -> embedding`.
* In `build_features_from_candidates` merge these embeddings into the candidate DataFrame as array columns or separate vector store.
* Compute `text_similarity` as row-wise dot product between the two embedding arrays (fast vectorized ops). Do not recompute SBERT for each candidate.

Example sketch:

```python
# Precompute unique texts and embeddings
unique_vehicle_texts = df[['vehicle_id','veh_brand','veh_model']].drop_duplicates()
unique_vehicle_texts['veh_text'] = ...
unique_vehicle_texts['veh_emb'] = compute_text_embeddings(unique_vehicle_texts['veh_text'].tolist())
# persist to disk
```

### D — Group sizes ordering

When you assemble training data for LGBMRanker, ensure the order and group sizes:

```python
# X_train, y_train are in final order
user_train = train_df['user_id'].reset_index(drop=True)
group_train = user_train.groupby(user_train, sort=False).size().tolist()
```

### E — Better fallback and normalization for profile similarity

Replace `sim = 0.5` with population mean similarity or zero after normalization, and make sure both profile vectors and vehicle vectors are normalized.

### F — Evaluation metrics and validation

Add:

* NDCG@K, MAP@K, Precision@K (K = 5 or 10 depending on product).
* Grouped cross-validation using `GroupKFold(n_splits=...)` on user_id.
* Baselines: brand-only ranking (brand_match desc -> random), popularity ranking (vehicles seen by many users), and a simple logistic regression as sanity check.

### G — Production / Inference design

* Precompute vehicle embeddings and index them with FAISS for fast nearest-neighbor to candidate vehicles or to compute profile similarities quickly.
* For inference per vehicle: compute the model features using merges and fast dot products; the expensive part is SBERT only if you need to encode new textual queries — but in many cases vehicle metadata is static; encode once.
* Serve LGBM model in a microservice. For latency-sensitive flows use precomputation + ANN + re-ranking pipeline:

  1. candidate generation (cheap heuristics, ANN on embeddings or filter by brand/budget)
  2. re-rank top-N using LGBMRanker features

---

# Suggested code patches (key excerpts)

1. **Change `build_features_from_candidates` signature**:

```python
def build_features_from_candidates(df: pd.DataFrame,
                                   historical_matches: pd.DataFrame = None,
                                   precomputed_pre_emb=None,
                                   precomputed_veh_emb=None) -> pd.DataFrame:
    """
    If historical_matches provided, use it to build user_profiles (no leakage).
    precomputed_*_emb are dicts or DataFrames mapping id -> embedding to avoid recompute.
    """
    ...
```

2. **Compute group sizes preserving order** (train_real):

```python
# after building X_train, aligned with train_df rows
user_train = train_df['user_id'].reset_index(drop=True)
group_train = user_train.groupby(user_train, sort=False).size().tolist()
```

---

# Feature engineering suggestions (to improve accuracy)

1. Location features: compute geographic distance (Haversine) between buyer desired location and vehicle location.
2. Budget-relative features: ratio `(price - price_mid) / price_mid`, or clipped log error.
3. Brand/model fuzzy matching: use normalized tokens and Levenshtein or token overlap as additional textual features.
4. Seller-related features: seller reputation, listing age, number of views — if available.
5. Time features: when the preorder was created vs listing posted — recency matters.
6. Interaction features: historic conversion rates per brand/model for that user.
7. Categorical embedding: encode brand and model with target encoding or learned embedding (for many brands/models).
8. Calibration: add `is_same_city` boolean.

---

# Evaluation & monitoring

* Use **grouped cross-validation** by user (GroupKFold).
* Use **NDCG@5, Precision@5, MAP@10** as primary metrics (choose K to match UI).
* Monitor offline vs online gap: do an A/B test in production, measure clicks, contact rates and conversion.
* Check calibration of ranking: check how often top-1 is true match.

---

# Scalability & infra

* SBERT: run on GPU for faster encoding or batch CPU with `batch_size`.
* Persist embeddings as `.npy` or Parquet with arrays (or use Milvus/FAISS).
* For candidate generation in prod: use lightweight filters (brand, price range, location) then ANN re-rank.
* Memory: avoid storing large arrays inside DataFrame columns when unnecessary — instead reference embedding indices.

---

# Quick checklist you can implement now (ordered)

1. **Fix leakage**: split users into train/test before profile building. (Critical)
2. **Modify feature builder**: accept `historical_matches` and precomputed embeddings. (High)
3. **Compute/precompute embeddings**: unique vehicles and unique preorders, persist. (High)
4. **Fix group ordering** when creating `group_train` and `group_test`. (Critical)
5. **Add evaluation metrics**: NDCG@5, MAP@K. Do GroupKFold CV. (High)
6. **Replace fallback** `0.5` by population mean or zero and normalize vectors. (Medium)
7. **Add location distance, recency features, and advanced price features**. (Medium)
8. **Profile weighting**: recency-weighted average for user profile. (Medium)
9. **Production plan**: candidate generation + re-rank pipeline + ANN. (Medium)

---

# Example: minimal fix for leakage + group sizes

Below is a minimal code sketch you can drop into `train_real.py` to avoid the biggest leakage:

```python
# after loading df
unique_users = df['user_id'].unique()
train_users, test_users = train_test_split(unique_users, test_size=test_size, random_state=RANDOM_STATE)

train_df = df[df['user_id'].isin(train_users)].reset_index(drop=True)
test_df  = df[df['user_id'].isin(test_users)].reset_index(drop=True)

# Build precomputed embeddings (recommended) or let build_features_from_candidates compute text features, but:
# Build user_profiles from only train matches
historical_matches = train_df[train_df['matched'] == 1]

X_train = build_features_from_candidates(train_df, historical_matches=historical_matches)
X_test  = build_features_from_candidates(test_df, historical_matches=historical_matches)

y_train = train_df['matched'].astype(int)
y_test  = test_df['matched'].astype(int)

# Ensure order preserved and then compute groups
user_train = train_df['user_id']
user_test = test_df['user_id']
group_train = user_train.groupby(user_train, sort=False).size().tolist()
group_test  = user_test.groupby(user_test, sort=False).size().tolist()
```

Also update `build_features_from_candidates` to use the `historical_matches` param for profile building rather than the incoming `df`.

---

# Final recommendations (practical priorities)

1. **First deploy fixes to avoid leakage** and compute user profiles from historical data only.
2. **Precompute embeddings** and redesign `build_features_from_candidates` to join embedding IDs rather than re-encode strings repeatedly.
3. **Add robust grouped CV** and evaluate with ranking metrics (NDCG@K). Compare to baseline (brand-match).
4. **Improve features** (location, recency, price transforms), tune LGBM hyperparameters via CV.
5. **For production**, design a candidate generation + ANN re-rank architecture for latency.

---

If you want, I can:

* produce a PR-style patch for the three files implementing the *minimal* fixes (user split before FE + group train size fix + function signature change) so you can run immediate experiments; **or**
* write a more extensive refactor that precomputes embeddings and implements efficient feature joins + FAISS re-ranking mock.

Which do you want me to implement now? (I can produce the code patch directly in this chat.)
